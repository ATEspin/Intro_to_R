---
title: "Working through an example"
author: "Abel Torres Espin"
date: 'March 2023'
output:
    prettydoc::html_pretty:
      toc: true
      theme: cayman
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This document will work through an example of a data science project in R, from organizing the project to full reporting with tables and graphics. This is a companion of the full course *Introduction to data science and biostatistics in R*. Each section will provide a working example of the material seen in the course.

We will work through the following workflow:

1.  Defining the project

2.  Project setup and organization

3.  Data import

4.  Exploratory data analysis and data wrangling

5.  Analytics

6.  Visualization and reporting

# Defining the project

Here a question you we might be interested to answer:

> **Does task-specific rehabilitative training improve forelimb function after cervical spinal cord injury?**

You could think about how to set a hypothesis and design the experiments to collect the data and investigate this question. Luckly for use, this has been studied before and we can use available data. We are going to work with a data from a study conducted by the Fouad and Fenrich labs on forelimb functional recovery in cervical spinal cord injury models:

> Lucas-Osma AM, Schmidt EKA, Vavrek R, Bennett DJ, Fouad K, Fenrich KK. Rehabilitative training improves skilled forelimb motor function after cervical unilateral contusion spinal cord injury in rats. Behav Brain Res. 2022 Mar 26;422:113731. doi: [10.1016/j.bbr.2021.113731](https://doi.org/10.1016/j.bbr.2021.113731). Epub 2021 Dec 31. PMID: 34979221.
>
> Schmidt E, Raposo P, Vavrek R, Fouad K. Inducing inflammation following subacute spinal cord injury in female rats: A double-edged sword to promote motor recovery. Brain Behav Immun. 2021 Mar;93:55-65. doi: [10.1016/j.bbi.2020.12.013](https://doi.org/10.1016/j.bbi.2020.12.013). Epub 2020 Dec 21. PMID: 33358981.

The dataset is publicly available through the Open Data Commons for Spinal Cord Injury ([odc-sci.org](https://odc-sci.org)). You can download both the dataset and the data dictionary at <https://odc-sci.org/data/747>, or use the files in the /*data* folder.

# Project organization

Before we start our data project, it is a good idea to define some structure for the files and folders. This helps to keep things organized and makes our project easier to reproduce and understand. In this section we will take care of the following:

1.  Starting an RStudio project

2.  Setting the folder project structure and files

3.  Initiate a dynamic document or notebook

## Starting an RStudio project

RStudio makes data projects very easy to manage with RStudio projects. These have a few features that makes the work more organized such as control of the environment and console history, or quick access to the project.

You can start a new project from *File/New project*... and follow the instructions. RStudio will create a project folder in the location you specify.

## Setting the project folder

Once RStudio has created the project folder, we may want to provide some subfolders and files to organize the progress of the project. This is a common tree folder structure I use in data projects. You can use your own, just make sure to be consistent and document the structure.

```{=asciidoc}
my_project/
├─ data/
│  ├─ original/
│  |  ├─ data.csv
│  ├─ derived/
|  |  ├─ new data.csv
├─ output/
│  ├─ figures/
│  ├─ tables/
├─ scripts/
│  ├─ my_functions.R
├─ documents/
│  ├─ project_summary.docx
├─ README.txt
├─ my_project.Rproj
├─ my_notebook.Rmd
```
`my_project`: the main (root) folder of your project. I usually keep my notebooks here together with a README file

`README.txt`: It is good practice to have a README file to explain details of the project. Common things you may want to document here is a short summary of the project and an explanation of the folders and files. Note that here is a \**.txt* file since they can be opened on any system, but any text based file would do the job.

`data`: contains all the data necessary for the project. It has two sub-folders, one for the `original` data and one for `derived` data generated during the work of the project. I like this structure because it isolates the starting data. In case of a mistake, I can always go back to the original files.

`output`: a folder containing the outputs from the code. These are usually in the form of plots, figures and tables, but it can be other types like \*.*html* files.

`scripts`: a folder with script files containing some useful code. For example, I use this when I code functions in a external file that I can then load into different notebooks in a project.

`documents`: contains different documents that may help with the understanding of the project. For example, for this project, we could download the \**.pdf* files of the originating publications for the data we are using.

## Initiate a dynamic document or notebook

Data science projects are dynamic and interactive, and they benefit from the combination of code, code outputs and narrative of our thinking process. We call these notebooks. They allow you to alternate regular text and chunks of code in a single document. This document is a notebook! **It is like a lab notebook but for your code!** In RStudio we can use *Rmarkdown* files as notebooks. Markdown is a markup language, that is, text with some extra things to format the text. Rmarkdown uses this to be able to have code and text in the same document. 

The great advantage of Rmarkdown is that greatly facilitate reporting, communicating the results and reproducibility.

### R Markdown

To start an Rmarkdown file, you can go to *File/New File/R Markdown...*. This will generate a file with some markdown text and code examples.

We can see three parts in a Rmarkdown file, the header (marked in between ---), markdown text and code chunks. The header is coded in something called YAML and provides some options to the file.

This is how a code chunk looks like:

````

```{r}`r ''`

# This is a code chunk in Rmarkdown. # Marks a comment

```

````

There are some organization and structure that we can have in the notebook to help readability. For instance, we can start the document with some explanatory text of what the document is about as we have here, followed by chunks of code that sets some initial parameters or loads libraries, etc.

Finally, you can render the notebook to an **.html* file by clicking on the `Kint` button under the file tab name. This can take a while if your code takes long to run.

# Data import

Now we are ready to import data! The data is in **.csv*, which makes things very easy for us.

How do we upload a **.csv* file to the R session? (programmatically)

```{r}
# Write code here to read the data and the data dictionary into two different objects

odc_df<-read.csv("data/odc-sci_747.csv")
odc_dic<-read.csv("data/odc-sci_747_dictionary.csv")
```

## Initial look at the data

It is important to check that the data is what we expect and that we understand the data.

```{r}
# write code here to check the column names of the data
colnames(odc_df)
colnames(odc_dic)

# Correct variable names if needed. In this example we set the first column name for the data dictionary to be "VariableName"
colnames(odc_dic)[1]<-"VariableName"
```

```{r}
# write code here to check the structure of the data and the data dictionary
str(odc_df)
```

```{r}
# write code here to check if all variables in the dataset are defined in the data dictionary.

## Hint. You can check in a character vector has matching elements with another character vector with char_vector_1 %in% char_vector_2 operation

VarNames<-colnames(odc_df) # save variable names in dataset

VarNames%in%odc_dic$VariableName #matches in logical vector

mean(VarNames%in%odc_dic$VariableName) #proportion of matches

VarNames[!VarNames%in%odc_dic$VariableName] #which ones do no match
```

**What observations can we make about the dataset?**

***

# Exploratory data analysis

Exploratory data analysis or EDA is the process for which we start gaining some insights into the dataset and the question at hand. It allows to gain further information on the content of the dataset, structure, and the data itself in relation to our question at hand. 

There are different methods that can be used for EDA, but in general we can divide them into descriptive statistics and visualizations.

## Framing the question

Now that we have the data uploaded to the site and understand its structure, we can frame the question that we want to answer in the context of the dataset.

Remember our main question is:

> **Does task-specific rehabilitative training improve forelimb function after cervical spinal cord injury?**

In the context of this dataset, we need to define what information we have on rehabilitative training, and what improvement means.

### Experimental groups

We know this is a designed experiment, so we expect different groups. The dataset includes the Group variable.

```{r}
# the unique function returns the different values in a vector
unique(odc_df$Group)
```

> What does it means to be in the group 1 or 2? We cannot find that information on the data dictionary. At this point, we are blinded to the group assignment, and we might want to contact the data authors for information.

We also need to be aware that there are different cohorts of animals in this dataset

```{r}
unique(odc_df$Cohort)

# the table function allows for a quick cross-tabulation of the counts of entries on two vectors
table(odc_df$Group, odc_df$Cohort)
```

We can see that all animals in cohort 3 belong to group 2, and that there is an unbalance distribution between groups in cohort 2

```{r}
# we can also add the total marginal counts to the table
addmargins(table(odc_df$Group, odc_df$Cohort))
```

### Response variables

The rehabilitative training performed in this study is the single pellet reaching, grasping and retrieval task. Looking at the dataset we can see that:

1. Time of measuring performance is encoded over columns in the variable name. We will need to transform this for analysis.
2. We have a few response variables that have been measured (number of attempts, percentage of reaching, percentage of grasping and percentage of retrieval). We may want to study those.

## Data pre-processing for analysis

**Come back after data wrangling lecture**

WORK IN PROGRESS HERE
